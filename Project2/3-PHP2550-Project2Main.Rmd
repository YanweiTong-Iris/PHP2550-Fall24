---
title: "Smoking cessation in adults with major depressive disorder (MDD)"
subtitle: "PHP2550 Project 2: A regression analysis"
author: "Yanwei (Iris) Tong"
date: "2024-10-10"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
    #number_sections: true
link-citations: yes
header-includes:
  - "\\usepackage{xcolor}"
abstract: |
  **Purpose:** "to examine baseline variables as potential moderators of the effects of behavioral treatment on end-of-treatment (EOT) abstinence and evaluate baseline variables as predictors of abstinence, controlling for behavioral treatment and pharmacotherapy"  
  
  **Methods**: Bidirectional Stepwise Selection, Elastic Net Regression, and Best Subset Selection     
  
  **Results and conclusion:**     
  

geometry: margin=1in
fontsize: 10.5pt
csl: 0-american-statistical-association.csl
bibliography: 0-references.bib
---



```{r setup, include=FALSE}
# to prevent scientific notation
options(scipen=999)

# Set up knit environment
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      error = FALSE)

# Load necessary packages
library(tidyverse)
library(kableExtra)
library(knitr)
library(ggplot2)
library(gridExtra)
library(grid)
library(naniar)
library(gtsummary)
library(gt)
library(patchwork)
library(stargazer)
library(knitcitations)
library(mosaic)
library(mice)
library(summarytools)
library(npreg)
library(mgcv)
library(glmnet)
library(pROC)
library(MASS)
library(survey)
library(glmulti)
library(leaps)
```

```{r}
# Define data path and import data
data_path = "/Users/yanweitong/Documents/PHP2550-Data/Project2"
data = read.csv(paste0(data_path, "/project2.csv"))

# Data preprocessing
data = data  %>%
  # create race variable
  mutate(race = factor(case_when(
    NHW == 1 ~ "Non-Hispanic white",
    Black == 1 ~ "Black",
    Hisp == 1 ~ "Hispanic",
    TRUE ~ "Other"  # Handle cases where none of the above conditions are met
  ), levels = c("Non-Hispanic white", "Black", "Hispanic", "Other"))) %>%
  # create treatment categories
  mutate(treatment_cat = factor(case_when(BA == 1 & Var == 0 ~ "BASC+placebo",
                                   BA == 0 & Var == 0 ~ "ST+placebo",
                                   BA == 1 & Var == 1 ~ "BASC+varenicline",
                                   BA == 0 & Var == 1 ~ "ST+varenicline"))) %>% 
  # factorize categorical/ordinal variables
  mutate(
    abst = factor(abst),
    Var = factor(Var),
    BA = factor(BA),
    sex_ps = factor(sex_ps),
    NHW = factor(NHW),
    Black = factor(Black),
    
    ftcd.5.mins = factor(ftcd.5.mins),
    otherdiag = factor(otherdiag),
    antidepmed = factor(antidepmed),
    mde_curr = factor(mde_curr),
    Only.Menthol = factor(Only.Menthol),
    edu = factor(edu, levels = c(1, 2, 3, 4, 5)),
    inc = factor(inc, levels = c(1, 2, 3, 4, 5))
  ) %>%
  # make integers numeric 
  mutate(across(
    .cols = where(is.integer) & !all_of("id"),
    .fns = as.numeric 
  ))
```

# \textcolor{orange}{INTRODUCTION}

This regression analysis project, conducted in collaboration with Dr. George Papandonatos from Brown Department of  Biostatistics, seeks to explore factors influencing smoking cessation among adults with major depressive disorder (MDD). Individuals with MDD often smoke more heavily, demonstrate stronger nicotine dependence, and experience more challenging withdrawal symptoms than those without MDD. While varenicline is a proven aid for smoking cessation, addressing psychological factors associated with MDD-related smoking behaviors might also improve quit rates in this population.

Dr. Papandonatos' previous randomized, placebo-controlled study (@hitsman2023efficacy), which included 300 adult smokers with either current or past MDD, employed a 2x2 factorial design and compared behavioral activation for smoking cessation (BASC) against standard treatment (ST) and varenicline versus placebo. The multi-center study found no significant differences in abstinence outcomes between BASC and ST, regardless of varenicline use. However, varenicline significantly outperformed placebo at the 27-week follow-up, achieving a quit rate of 16.2\% compared to 7.5\% for the placebo group. 

Building on the original study, this project will reexamine data from the same trial to accomplish the following two main objectives: 1) identify baseline variables that may moderate the impact of behavioral treatments on end-of-treatment (EOT) smoking abstinence, and 2) evaluate baseline variables as predictors of abstinence outcomes, accounting for the effects of both behavioral treatments and pharmacotherapy. 

# \textcolor{orange}{DATA}

## Data Overview 

The study population of the RCT consists of 300 adult smokers with a history of current or past MDD. 1) Sociodemographic, 2) smoking-related, and 3) psychiatric characteristics of the participants were collected at baseline and displayed in **Table 1**. Demographics are age, sex, race, income, education; the smoking behaviors/measurements include key statistics like Fagerstrom Test for Cigarette Dependence (FTCD) score, Nicotine Metabolism Ratio (NMR), and indicator for exclusive Mentholated cigarette user; and psychiatric disgnotic and treatment history . The participants were randomized into four groups: BASC with placebo (`BASC+placebo`), BASC with varenicline (`BASC+Varenicline`), ST with placebo (`ST+placebo`), and ST with varenicline (`ST+Varenicline`). 

\newpage
\begin{landscape}
```{r}
# for sub-tab purpose
table1_data = data %>%
  mutate(
    Demographics = NA,
    Smoking = NA,
    Psychiatric = NA
  ) %>%
  mutate(edu = factor(edu, levels = c(1, 2, 3, 4, 5),
                 labels = c("Grade school", 
                            "Some high school", 
                            "High school graduate or GED", 
                            "Some college/technical school", 
                            "College graduate")),
    inc = factor(inc, levels = c(1, 2, 3, 4, 5),
                 labels = c("Less than $20,000", 
                            "$20,000–35,000", 
                            "$35,001–50,000", 
                            "$50,001–75,000", 
                            "More than $75,000")))
  
table1_data %>%
  dplyr::select(
    treatment_cat,
    Demographics,
    age_ps,
    sex_ps,
    race,
    inc,
    edu,
    Smoking,
    cpd_ps,
    ftcd_score,
    ftcd.5.mins,
    bdi_score_w00,
    crv_total_pq1,
    hedonsum_n_pq1,
    hedonsum_y_pq1,
    NMR,
    Only.Menthol,
    readiness,
    Psychiatric,
    shaps_score_pq1,
    otherdiag,
    antidepmed,
    mde_curr
  ) %>%
  tbl_summary(
    statistic = list(all_continuous() ~ c("{mean} ({sd})"),
                     all_categorical() ~ "{n} ({p}%)"),
    by = treatment_cat,
    digits = all_continuous() ~ 1,
    missing = "no",
    type = list(
      age_ps ~ "continuous",
      sex_ps ~ "dichotomous",
      race ~ "categorical",
      inc ~ "categorical",
      edu ~ "categorical",
      cpd_ps ~ "continuous", 
      ftcd_score ~ "continuous",
      ftcd.5.mins ~ "dichotomous",
      bdi_score_w00 ~ "continuous",
      crv_total_pq1 ~ "continuous",
      hedonsum_n_pq1 ~ "continuous",
      hedonsum_y_pq1 ~ "continuous",
      NMR ~ "continuous",
      Only.Menthol ~ "dichotomous",
      readiness ~ "continuous",
      shaps_score_pq1 ~ "continuous", 
      otherdiag ~ "dichotomous",
      antidepmed ~ "dichotomous",
      mde_curr ~ "dichotomous"
    ),
    label = list(
      age_ps = "Age (years)",
      sex_ps = "Sex (female)",
      race = "Race",
      inc = "Income",
      edu = "Education",
      cpd_ps = "Cigarettes per day",
      ftcd_score = "FTCD score",
      ftcd.5.mins = "Smoking with 5 mins of waking up (Yes)",
      bdi_score_w00 = "BDI score",
      crv_total_pq1 = "Cigarette reward value",
      hedonsum_n_pq1 = "Pleasurable Events Scale (substitute reinforcers)",
      hedonsum_y_pq1 = "Pleasurable Events Scale (complementary reinforcers)",
      Only.Menthol = "Exclusive mentholated cigarette user (Yes)",
      readiness = "Readiness to quit smoking",
      NMR = "Nicotine Metabolism Ratio",
      shaps_score_pq1 = "Anhedonia", 
      otherdiag = "Other lifetime DSM-5 diagnosis (Yes)",
      antidepmed = "Antidepressant medication (Yes)",
      mde_curr = "Current (and past) MDD vs past MDD only (Yes)"
    ),
    value = list(
      sex_ps ~ "2",
      Only.Menthol ~ "1",
      otherdiag ~ "1",
      antidepmed ~ "1",
      mde_curr ~ "1",
      ftcd.5.mins ~ "1"
    )
  ) %>%
  add_overall() %>%
  add_p() %>%
  modify_header(label ~ "**Characteristic**") %>%
  modify_caption(caption = "Participant characteristics by overall sample and treatment arm") %>%
  # for sub-tab purpose
  modify_table_body(
    ~ .x %>%
      mutate(across(everything(), ~ ifelse(. == "0 (NA%)", "", .)))
  ) %>%  
  modify_table_styling(
    rows = label %in% c("Sociodemographics", "Smoking", "Psychiatric"),
    columns = label,
    text_format = "bold"
  )   %>%
  as_kable_extra(
    booktabs = TRUE,
    longtable = TRUE,
    linesep = "",
    format = "latex"
  ) %>%
  kableExtra::kable_styling(
    position = "center",
    latex_options = c("striped", "repeat_header", "hold_position", "scale_down"),
    stripe_color = "gray!15",
    font_size = 8
  )
```

\end{landscape}
\newpage


Overall, the randomization process appears successful, as key variables such as demograohic characteristics, smoking intensity (cigarettes per day), and psychiatric measures like the DSM-5 diagnosis and anhedonia scores showed similar distributions across the four treatment arms with *p*-value much greater than 5\%, suggesting that participant characteristics were well-balanced across treatment arms, as expected in an RCT. This balance across groups reinforces the original study's internal validity, as any differences in outcomes can be more confidently attributed to the interventions rather than baseline differences in participant characteristics.

Regarding education, as shown in **Table 1**, two of the lowest education levels— grade school and some high school— had very few participant counts (1 and 16, respectively). To ensure adequate sample size and meaningful comparisons across education categories, we merged the first three levels (grade school, some high school, and high school graduate or GED) into a single category, considered as "High School and Below." This aggregation would improve the interpretability of the data by creating a more substantial subgroup and reducing variability, allowing for more reliable statistical analyses.



```{r}
# Create a new variable that contains the 3 first levels of edu
data <- data %>%
  mutate(edu_merged = factor(case_when(
    edu %in% c("1", "2", "3") ~ "1",
    edu == "4" ~ "2",
    edu == "5" ~ "3"
  )))

```


```{r}
# Create and display the contingency table between race and menthol useage
table_race_menthol <- table(data$race, data$Only.Menthol)

# Perform the chi-square test
chi_square_test <- chisq.test(table_race_menthol)

chi_square_text <- paste0(
  sprintf("Chi-Square Statistic ≈ %.2f", chi_square_test$statistic),
  sprintf("， p-value ≈ %.4f", chi_square_test$p.value)
)

kable(table_race_menthol, 
      caption = "Contingency Table of Race vs. Only Menthol Use with Chi-Square Test Result", 
      col.names = c("Non-Menthol-Only", "Menthol-Only"),
      row.names = TRUE) %>%
   footnote(chi_square_text,
            footnote_as_chunk = FALSE)

```
## Data Missingness and Imputation

```{r}
# Missingness table
# Calculate missing values for each variable
missing_summary <- data %>%
  summarise(across(everything(), ~ sum(is.na(.)), .names = "{col}")) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Number") %>%
  mutate(Pct = (Number / nrow(data)) * 100) %>%
  filter(Number > 0)  # Exclude variables with 0 missingness

# Define a named vector with old and new names for variables
variable_names <- c(
  "inc" = "Income",
  "ftcd_score" = "FTCD score at baseline",
  "crv_total_pq1" = "Cigarette reward value at baseline",
  "shaps_score_pq1" = "Anhedonia",
  "NMR" = "Nicotine Metabolism Ratio",
  "Only.Menthol" = "Exclusive Mentholated Cigarette User",
  "readiness" = "Baseline readiness to quit smoking"
)

# Rename variables in the summary table
missing_summary <- missing_summary %>%
  mutate(Variable = recode(Variable, !!!variable_names))


# Calculate total missing values and total missing percentage
total_rows_with_missing <- sum(rowSums(is.na(data)) > 0)
total_rows_with_missing_pct <- (total_rows_with_missing / nrow(data)) * 100


missing_summary <- missing_summary %>%
  arrange(desc(Pct)) %>%
  mutate(Pct = sprintf("%.2f%%", Pct)) 

# Combine the total missingness row with the summary table
total_missing_row <- tibble(
  Variable = "Participants with any missingness",
  Number = total_rows_with_missing,
  Pct = sprintf("%.2f%%", total_rows_with_missing_pct)
)

missing_summary <- bind_rows(total_missing_row, missing_summary)

# Display the final table
missing_summary %>%
  kable(
    col.names = c("Variable", "Number", "Pct"), 
    caption = "Summary of Missing Values"
  )
```

The overall missingness in the dataset is approximately 20\%, which exceeds the commonly accepted 5\% threshold for data completeness. This level of missing data could compromise the validity of the analysis if left unaddressed, as it may lead to biased results or loss of valuable information. To preserve the dataset's integrity and maintain statistical power, we opted to impute the missing values. We utilized the Multiple Imputation by Chained Equations (MICE) method, which allows for flexible handling of various data types and patterns of missingness, thereby maximizing the use of available information and improving the robustness of the analyses.

```{r}
# Perform MICE imputation
data_mice <- mice(data, m = 5, method = "pmm", 
                  maxit = 50, seed = 2024, printFlag = FALSE)


# Complete the data by extracting one of the imputed datasets
data_imp <- complete(data_mice, action = 1) 
# and calculate mean for numeric and mode for categorical/binary columns
# data_long <- complete(data_mice, action = "long") 

# Define a mode function for categorical/binary variables
# mode_func <- function(x) {
#   ux <- unique(x)
#   ux[which.max(tabulate(match(x, ux)))]
# }

# data_imp = data_long %>%
#   dplyr::select(-.id, -.imp) %>%
#   group_by(id) %>%
#   summarize(across(where(is.numeric), mean, na.rm = TRUE), 
#             across(where(~ is.factor(.) || is.character(.)), mode_func)) %>%
#   ungroup() %>%
#   mutate(across(where(is.numeric) &
#                   !matches("NMR"), # NMR is not integer
#                 round))
```




# \textcolor{orange}{METHODS}

## Logistic Regression

Given that our outcome variable `abst` is dichotomous, logistic regression is an appropriate modeling choice for this project. Logistic regression models the log-odds of the outcome as a linear combination of predictor variables, as expressed by the equation
$$
\log\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \beta_{ij} x_i x_j + \beta_{ik} x_i x_k + \dots
$$
where the $x_i x_j$ terms denote potential interaction terms (explained in the next subsection).

This transformation ensures that predicted probabilities remain within the range of 0 to 1. To facilitate interpretation, we exponentiate the coefficients ($exp(\beta)$) to express them as odds ratios, which indicate the multiplicative change in the odds of the outcome for each one-unit increase in a predictor, holding other variables constant. When testing the selected model on the test dataset, the `predict()` function in R with `type = "response"` outputs probabilities directly, using the equation
$$
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \beta_{ij} x_i x_j + \beta_{ik} x_i x_k + \dots)}}
$$
This probability-based output provides an intuitive interpretation of the model's predictions in terms of the likelihood of the event occurring given the predictors.


## Variable Inclusion Criteria for Full Models

For objective 1, psychotherapy (`BA`) is the primary predictor of interest, while pharmacotherapy (`Var`) is treated as a control variable. Thus, we ensured to include both of the treatments in all models as consistent factors. The goal of this analysis is to explore how various baseline characteristics might moderate the effect of behavioral treatment on smoking cessation/abstinence outcomes. To achieve this, we included a range of interaction terms involving `BA` and key sociodemographic, smoking-related, and psychiatric variables as potential moderators. These interaction terms would allow us to examine how different participant characteristics may influence the effectiveness of behavoriol activation in promoting abstinence. However, we excluded certain terms, such as interactions between `BA` and variables like indicator of smoking with 5 mins of waking up (`ftcd.5mins`) or cigarettes per day (`cpd_ps`), because these measures are components of the broader FTCD score (`ftcd_score`) and thus would provide redundant information. By prioritizing unique, non-overlapping terms, we aimed to create a comprehensive yet parsimonious model.

In addition to `BA` interaction terms as potential moderators, we included other interaction terms involving `Var` and various baseline characteristics as covariates. We belived these covariate interaction terms shall be considered important, as they account for known associations that could impact treatment outcomes. For instance, as previously discussed, there was a recognized relationship between race and menthol-only cigarette use, which could affect smoking cessation success. Similarly, the synergy between factors like baseline readiness to quit smoking (`readiness`), Nicotine Metabolism Ratio (`NMR`), and MDD history (`mde_curr`) could have critical influences smoking behavior and mental health status and have plausible biological or statistical interactions with other sociodemographic and smoking-related variables. These interactions reflect meaningful covariate effects that contribute to a more nuanced understanding of how different factors influence treatment outcomes, enabling a more robust analysis of predictors and moderators in the context of smoking cessation.

For objective 2, the focus shifts to using baseline variables solely as predictors of smoking cessation outcomes, rather than as covariates or moderators. In this context, `BA` and `Var` are included as control variables across all models to account for the effects of behavioral and pharmacotherapy interventions. However, our goal here is to examine the predictive power of baseline characteristics independently, so we do not include any interaction terms. This approach simplifies the model by isolating the main effects of each baseline variable without adding complexity through interactions.

## Model Selection
### 1) Bidirectional Stepwise Selection
Bidirectional stepwise selection is a model selection method that combines both forward selection and backward elimination to identify the most significant predictors for inclusion in the regression model. In forward selection, variables are added one at a time based on specific criteria, while backward elimination starts with all candidate variables and removes the least significant ones iteratively. By using both directions, this approach benefits from the advantages of both methods, allowing for the addition of meaningful variables and the removal of redundant ones, which can result in a more optimal and parsimonious model.

In our analysis, we employed bidirectional stepwise selection using the Akaike Information Criterion (AIC) as the selection criterion. AIC balances model fit and complexity by penalizing the number of parameters, helping to prevent overfitting. The bidirectional approach enhances the model selection process by re-evaluating the significance of variables at each step, ensuring that the final model includes only those predictors that provide substantial contributions to explaining the variability in the outcome.

### 2) Elastic Net Regression
Elastic Net regression is a regularization and variable selection technique that incorporates both LASSO (L1 penalty) and Ridge (L2 penalty) regression methods. It addresses some limitations of LASSO, particularly in situations where predictors are highly correlated. By combining L1 and L2 penalties, Elastic Net encourages a grouping effect where correlated variables tend to be selected or excluded together, leading to more stable and reliable models.

In our modeling, Elastic Net was utilized to take advantage of both variable shrinkage and selection properties of LASSO and the grouping effect of Ridge regression. This method helps in handling multi-collinearity among predictors while performing variable selection. We used cross-validation to determine the optimal value of the regularization parameter lambda, specifically selecting the lambda that minimizes the cross-validation error (lambda min). This approach ensures that the model achieves the best predictive performance on unseen data by effectively balancing bias and variance.

### 3) Best Subset Selection 
Best subset selection is a comprehensive model selection method that involves evaluating all possible combinations of predictor variables to identify the model that best fits the data according to a chosen criterion. 

For *Objective 1*, due to the complexity of the full model— which included a high number of potential interaction covariate terms— we employed the sequential replacement method, which is a more computationally efficient alternative to exhaustive search for complex models by iteratively replacing variables to find a subset that provides a good balance between model fit and complexity. For *Objective 2*, the model was less complex as it included only baseline predictors without interaction terms. This allowed us to use an exhaustive search strategy, evaluating all possible subsets of the predictors to find the optimal model. 

We selected models based on Mallow's $Cp$ criterion, aiming to minimize this statistic. Mallow's $Cp$ provides a measure of the model's predictive error relative to the number of predictors used, helping to select models that are both accurate and parsimonious. By minimizing Mallow's $Cp$, we ensured that the selected model offers the best trade-off between goodness of fit and model simplicity.

## Performance Metrics (Calibration and Discrimination)

To evaluate model performances, we used a combination of calibration and discrimination measures. Calibration plots with error bars and LOESS smoothing allowed us to visually assess the agreement between predicted probabilities and observed outcomes, indicating how well-calibrated the model is across different probability levels. The addition of error bars provides insights into the variability of predictions, while LOESS smoothing offers a flexible fit to better capture trends in calibration. The ROC curve evaluates the model's discrimination ability, reflecting its capability to distinguish between positive and negative outcomes. Furthermore, quantitative metrics such as Brier score and calibration error were included in tables to provide objective assessments of model accuracy and calibration. Brier score combines both calibration and sharpness of probability estimates, while calibration error specifically measures the deviation between predicted and observed probabilities, providing complementary insights into model performance beyond visual assessments.

# \textcolor{orange}{RESULTS}

## Baseline Characteristics as Potential Moderators 
```{r}
# Logistic regression with stepwise selection
# Define the outcome and variables in the model
outcome <- data_imp$abst
variable_names <- c("Var", "BA", "age_ps", "sex_ps", "inc", "edu_merged", "race",
                     "ftcd_score", "ftcd.5.mins", "bdi_score_w00", "cpd_ps",
                     "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",
                     "shaps_score_pq1", "otherdiag", "antidepmed", "mde_curr",
                     "NMR", "Only.Menthol", "readiness")
variables <- data_imp[, variable_names]
# for Lasso (to break down factors with >2 levels)
variables_dummy <- model.matrix(~ 0 + ., data = variables)
# remove the extra reference group
variables_dummy <- variables_dummy[, -which(colnames(variables_dummy) =="Var0")]

# Split into train and test
set.seed(1)
train_index <- sample(1:nrow(data_imp), 0.7 * nrow(data_imp))
train_data <- data_imp[train_index,]
test_data <- data_imp[-train_index,]
train_outcome <- outcome[train_index]
test_outcome <- outcome[-train_index]
# for best subset
train_variables <- variables[train_index, ]
test_variables <- variables[-train_index, ]


# Main effects for `Var` and `BA` (to be considered/controlled in all models)
main_effects <- paste(variable_names, collapse = " + ")

# Define the interaction terms we want to consider
BA_interaction_terms <- 
  paste("BA:Var", #interaction between treatments
        #interaction with demographics
        "BA:age_ps + BA:sex_ps + BA:inc+ BA:edu_merged + BA:race",
        # with smoking
        "BA:ftcd_score + BA:Only.Menthol + BA:NMR+ BA:readiness",
        # with MDD
        "BA:mde_curr + BA:bdi_score_w00 + BA:shaps_score_pq1 + BA:antidepmed",
        "BA:otherdiag + BA:shaps_score_pq1",
        sep = " + ")

other_interaction_terms <- paste(
  # pharmatherapy with demographics
  "Var:age_ps + Var:sex_ps + Var:race + Var:ftcd_score + Var:cpd_ps",
  # between demographics
  "inc:edu_merged",
  # Menthol exclusive with demographics
  "sex_ps:Only.Menthol + race:Only.Menthol + inc:Only.Menthol + edu_merged:Only.Menthol",
  # NMR with demographics, cigarette dependence, and readiness to quit
  "sex_ps:NMR + age_ps:NMR + cpd_ps:NMR + NMR:readiness + ftcd_score:NMR",
  # readiness to quit with cigarette dependence and MDD
  "ftcd_score:readiness + Only.Menthol:readiness + mde_curr:readiness ",
  # cigarette dependence with demographics
  "sex_ps:ftcd_score + race:ftcd_score + age_ps:ftcd_score",
  sep = " + "
)

# Full formula for main effects and interactions
full_formula <- as.formula(paste("abst ~", main_effects, "+", BA_interaction_terms,
                                 "+", other_interaction_terms))


# Define scope with `Var` and `BA` as forced terms in the main model
scope_list <- list(
  lower = as.formula("abst ~ Var + BA"),  # Minimal model with controlled terms
  upper = full_formula                    # Full model with all main and interaction terms
)

# Fit logistic regression model with stepwise selection
stepwise_model <- step(
  glm(formula = abst ~ Var + BA, data = train_data, family = "binomial"),
  scope = scope_list,
  direction = "both",
  trace = 0
)
```



```{r}
# for L0 + L1
train_variables_dummy <- variables_dummy[train_index, ]
test_variables_dummy <- variables_dummy[-train_index, ]

# Enforce Var and BA as 0 penalty
# Elastic Net
# ^2 generates all pairwise interactions
train_variables_dummy_df <- as.data.frame(train_variables_dummy)
train_variables_dummy_full_interactions <- model.matrix(~ .^2, 
                                                         data = train_variables_dummy_df)

test_variables_dummy_df <- as.data.frame(test_variables_dummy)
test_variables_dummy_full_interactions <- model.matrix(~ .^2, 
                                                         data = test_variables_dummy_df)

# To identify the potential interaction terms for moderator effects
train_variables_dummy_include_names <- c(
  "Var1", "BA1", "age_ps", "sex_ps2", "inc2", "inc3", 
  "inc4", "inc5", "edu_merged2", "edu_merged3",
  "raceBlack", "raceHispanic", "raceOther", 
  "ftcd_score", "ftcd.5.mins1", "bdi_score_w00", "cpd_ps",
  "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",              
  "shaps_score_pq1", "otherdiag1", "antidepmed1",                  
  "mde_curr1", "NMR", "Only.Menthol1",                 
  "readiness", "Var1:BA1", 
  #Behavioral treatment (MAIN)
  "BA1:mde_curr1", 
  "BA1:age_ps", "BA1:sex_ps2", 
  "BA1:raceBlack", "BA1:raceHispanic",
  "BA1:raceOther", "BA1:ftcd_score", 
  "BA1:shaps_score_pq1","BA1:bdi_score_w00", 
  "BA1:otherdiag1", "BA1:antidepmed1",
  "BA1:mde_curr1","BA1:NMR",
  "BA1:Only.Menthol1", "BA1:readiness",
  # Pharmatherapy
  "Var1:mde_curr1",
  "Var1:age_ps", "Var1:sex_ps2",
  "Var1:raceBlack", "Var1:raceHispanic",
  "Var1:raceOther", "Var1:ftcd_score", 
  # Income*Edu
  "inc2:edu_merged2", "inc2:edu_merged3", 
  "inc3:edu_merged2", "inc3:edu_merged3", 
  "inc4:edu_merged2", "inc4:edu_merged3", 
  "inc5:edu_merged2", "inc5:edu_merged3", 
  # Readiness to quit
  "Only.Menthol1:readiness", 
  "mde_curr1:readiness", "ftcd_score:readiness", 
  # FTCD Score 
  "sex_ps2:ftcd_score", "raceBlack:ftcd_score", 
  "raceHispanic:ftcd_score", "raceOther:ftcd_score", 
  "age_ps:ftcd_score", 
  # Menthol exclusive
  "sex_ps2:Only.Menthol1", "raceBlack:Only.Menthol1", 
  "raceHispanic:Only.Menthol1", "raceOther:Only.Menthol1", 
  "inc2:Only.Menthol1", "inc3:Only.Menthol1", 
  "inc4:Only.Menthol1", "inc5:Only.Menthol1", 
  "edu_merged2:Only.Menthol1", "edu_merged3:Only.Menthol1", 
  # NMR
  "sex_ps2:NMR", "age_ps:NMR", "cpd_ps:NMR", 
  "NMR:readiness", "ftcd_score:NMR"
)

train_variables_dummy_include = 
  train_variables_dummy_full_interactions[,train_variables_dummy_include_names]
test_variables_dummy_include = 
  test_variables_dummy_full_interactions[,train_variables_dummy_include_names]

# Set penalty factors to enforce keeping Var and BA
# Initialize penalty factors to 1 for all variables
penalty_factors <- rep(1, ncol(train_variables_dummy_include))

# Identify columns corresponding exactly to "Var1" and "BA1" (not their interactions)
var1_col <- grep("^Var1$", colnames(train_variables_dummy_include))
ba1_col <- grep("^BA1$", colnames(train_variables_dummy_include))

penalty_factors[c(var1_col, ba1_col)] <- 0
names(penalty_factors) <- colnames(train_variables_dummy_include)
```



```{r}
# Fit Elastic Net
set.seed(2024)
enet_model <- cv.glmnet(as.matrix(train_variables_dummy_include), train_outcome,
                         penalty.factor = penalty_factors,
                         alpha = 0.5, family = "binomial")

# Extract coefficients at the optimal lambda (best_lambda)
best_lambda_enet <- enet_model$lambda.min
#remove intercept
optimal_coefs_enet <- as.numeric(coef(enet_model, s = best_lambda_enet)[-1])
coef_names_enet <- rownames(coef(enet_model, s = best_lambda_enet))[-1]  

result_table_enet <- data.frame(
  variable = coef_names_enet,
  Coefficient = optimal_coefs_enet
) %>%
  filter(Coefficient != 0) 
```


```{r, results = "hide"}
set.seed(2024)
regsubsets_model <-
    suppressWarnings(regsubsets(
      y = train_outcome,
      x = train_variables_dummy_include,
      nbest = 1, # 1 best model for each number of predictors
      nvmax = 20,
      force.in = c("Var1", "BA1"),
      force.out = NULL,
      really.big = TRUE,
      method = "seqrep",
      warn.dep = FALSE
    ))

reg_summary = summary(regsubsets_model)
cp_min = which.min(reg_summary$cp)
best_subset_coefs = coef(regsubsets_model, cp_min)
best_subset_names = names(coef(regsubsets_model, cp_min))
```



```{r}
# Summary table of coef
# Stepwise coefficients
stepwise_coefs <- coef(stepwise_model)
stepwise_df <- data.frame(
  variable = names(stepwise_coefs),
  `Stepwise` = as.numeric(stepwise_coefs)
)

# ENET coefficients 
enet_df <- result_table_enet %>%
  rename(`Elastic Net` = Coefficient)

# Best subset coefficients
best_subset_df <- data.frame(
  variable = names(best_subset_coefs),
  `Best Subset` = as.numeric(best_subset_coefs)
)

# Merge all into one table based on variable names
combined_df <- full_join(stepwise_df, enet_df, by = "variable") %>%
  full_join(best_subset_df, by = "variable") %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

# Remove the intercept row
combined_df <- combined_df[combined_df$variable != "(Intercept)", ]

colnames(combined_df) <- gsub("Best.Subset", "Best Subset", colnames(combined_df))
```

```{r}
combined_df <- combined_df %>%
  mutate(
    variable = sapply(variable, function(x) {
      # Split each interaction term into components
      terms <- unlist(strsplit(x, ":"))
      # Sort alphabetically and rejoin with ":"
      if (length(terms) > 1) {
        paste(sort(terms), collapse = ":")
      } else {
        x  # If not an interaction term, keep it as is
      }
    })
  )

combined_df <- combined_df %>%
  mutate(across(c(Stepwise, `Elastic Net`, `Best Subset`), 
                ~ as.numeric(as.character(.))))


# Combine rows with the same standardized interaction term names
# Summing the coefficients for duplicate terms (or use mean instead if averaging is preferred)
combined_df <- combined_df %>%
  group_by(variable) %>%
  summarize(across(c(`Stepwise`, `Elastic Net`, `Best Subset`), sum, na.rm = TRUE)) %>%
  ungroup()

# Replace NA values with an empty space
combined_df <- combined_df %>%
  mutate(across(c(Stepwise, `Elastic Net`, `Best Subset`), ~ ifelse(. == 0, " ", .)))


# Display the final combined table
kable(combined_df, row.names = F,
      caption = "Summary of Coefficients for Potential Moderator Effects 
      across Model Selection Methods")
```


```{r}
predicted_prob_stepwise <- as.numeric(predict(stepwise_model,
                                newdata = test_data,
                                type = "response"))

# Plot ROC and calculate AUC
roc_stepwise <- roc(test_outcome, predicted_prob_stepwise)

roc_data <- data.frame(
  Specificity = rev(roc_stepwise$specificities),
  Sensitivity = rev(roc_stepwise$sensitivities)
)

# Calculate the AUC
auc_value <- auc(roc_stepwise)

# Plot ROC curve with ggplot2
ROC_stepwise = ggplot(roc_data, aes(x = 1-Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, 
           label = paste("AUC =", round(auc_value, 2)), size = 5, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5))
```

```{r}
num_cuts <- 10  # Number of bins for calibration

calib_data <- data.frame(
  prob = predicted_prob_stepwise,  # predicted probabilities
  # binning into `num_cuts` groups
  bin = cut(predicted_prob_stepwise, breaks = num_cuts),  
  # observed values (abst outcome in test data)
  class = as.numeric(test_outcome)-1  
)

calib_data <- calib_data %>%
  group_by(bin) %>%
  summarise(
    observed = mean(class),  
    predicted = mean(prob),  
    se = sqrt(observed * (1 - observed) / n())  # Standard error
  )

# Add Loess Fit for Flexible Calibration Line
loess_fit <- loess(observed ~ predicted, data = calib_data, span = 0.75)
calib_data$loess_pred <- predict(loess_fit, calib_data$predicted)

# Plot Calibration Curve with Error Bars
calib_error_bar_stepwise = ggplot(calib_data) + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  geom_errorbar(aes(x = predicted, ymin = observed - 1.96 * se, 
                    ymax = observed + 1.96 * se), 
                colour="black", width=.01)+
  geom_point(aes(x = predicted, y = observed)) +
  labs(x = "Expected Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  theme_minimal()


# Plot Calibration Curve with Loess
calib_data <- calib_data %>%
  mutate(loess_ci_lower = loess_pred - 1.96 * sd(loess_pred),
         loess_ci_upper = loess_pred + 1.96 * sd(loess_pred))

calib_loess_stepwise = ggplot(calib_data, aes(x = predicted, y = observed)) +
  # Flexible calibration (Loess)
  geom_line(aes(y = loess_pred), color = "blue", linetype = "dashed") +  
  geom_ribbon(aes(ymin = loess_ci_lower, ymax = loess_ci_upper), alpha = 0.2, fill = "grey") +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Perfect calibration line
  labs(x = "Predicted Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r}
predicted_prob_enet <- as.numeric(predict(enet_model, 
                                newx = as.matrix(test_variables_dummy_include), 
                                s = "lambda.min", type = "response"))

# Plot ROC curve and calculate AUC
roc_enet <- roc(test_outcome, predicted_prob_enet)

roc_data <- data.frame(
  Specificity = rev(roc_enet$specificities),
  Sensitivity = rev(roc_enet$sensitivities)
)

auc_value <- auc(roc_enet)

# Plot ROC curve with ggplot2
ROC_enet = ggplot(roc_data, aes(x = 1-Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, label = paste("AUC =", round(auc_value, 2)), size = 5, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

```{r}
num_cuts <- 10  # Number of bins for calibration

calib_data <- data.frame(
  prob = predicted_prob_enet,  # predicted probabilities
  # binning into `num_cuts` groups
  bin = cut(predicted_prob_enet, breaks = num_cuts),  
  # observed values (abst outcome in test data)
  class = as.numeric(test_outcome)-1  
)

calib_data <- calib_data %>%
  group_by(bin) %>%
  summarise(
    observed = mean(class),  
    predicted = mean(prob),  
    se = sqrt(observed * (1 - observed) / n())  # Standard error
  )

# Add Loess Fit for Flexible Calibration Line
loess_fit <- loess(observed ~ predicted, data = calib_data, span = 0.75)
calib_data$loess_pred <- predict(loess_fit, calib_data$predicted)

# Plot Calibration Curve with Error Bars
calib_error_bar_enet = ggplot(calib_data) + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  geom_errorbar(aes(x = predicted, ymin = observed - 1.96 * se, 
                    ymax = observed + 1.96 * se), 
                colour="black", width=.01)+
  geom_point(aes(x = predicted, y = observed)) +
  labs(x = "Expected Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  #       title = "Calibration Plot for Elastic Net Model with Error Bars"
  theme_minimal()


# Plot Calibration Curve with Loess
calib_data <- calib_data %>%
  mutate(loess_ci_lower = loess_pred - 1.96 * sd(loess_pred),
         loess_ci_upper = loess_pred + 1.96 * sd(loess_pred))

calib_loess_enet = ggplot(calib_data, aes(x = predicted, y = observed)) +
  # Flexible calibration (Loess)
  geom_line(aes(y = loess_pred), color = "blue", linetype = "dashed") +  
  geom_ribbon(aes(ymin = loess_ci_lower, ymax = loess_ci_upper), alpha = 0.2, fill = "grey") +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Perfect calibration line
  scale_color_manual(values = c("Ideal" = "red", 
                                "Flexible calibration" = "blue")) +
  scale_linetype_manual(values = c("Ideal" = "solid", 
                                   "Flexible calibration" = "dashed")) +
  labs(x = "Predicted Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence",
       color = "Legend", linetype = "Legend") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) 

```

```{r}
predict_best_subset <- function(train_data, test_data, best_subset_coefs) {
  train_data <- as.data.frame(train_data)
  test_data <- as.data.frame(test_data)
  
  # Extract variable names from best subset selection
  selected_vars <- names(best_subset_coefs)[-1]  # Exclude intercept
  formula <- as.formula(paste("abst ~", paste(selected_vars, collapse = " + ")))
  
  # Fit a glm model with the specified variables
  model <- glm(formula, data = train_data, family = binomial)
  
  # Predict on test data using type = "response" to get probabilities
  predicted_probabilities <- predict(model, newdata = test_data, type = "response")
  
  return(predicted_probabilities)
}

predicted_prob_best_subset <-  as.numeric(
  predict_best_subset(
    train_data = cbind(train_variables_dummy_include, 
                       abst = as.numeric(train_outcome) -1),
    test_data = cbind(test_variables_dummy_include, 
                      abst = as.numeric(test_outcome) -1),
    best_subset_coefs
  )
)

# Plot ROC and calculate AUC
roc_best_subset <- roc(test_outcome, predicted_prob_best_subset)

roc_data <- data.frame(
  Specificity = rev(roc_best_subset$specificities),
  Sensitivity = rev(roc_best_subset$sensitivities)
)

auc_value <- auc(roc_best_subset)

# Plot ROC curve with ggplot2
ROC_best_subset = ggplot(roc_data, aes(x = 1-Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, 
           label = paste("AUC =", round(auc_value, 2)), size = 5, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5))
```

```{r}
num_cuts <- 10  # Number of bins for calibration

calib_data <- data.frame(
  prob = predicted_prob_best_subset,  # predicted probabilities
  # binning into `num_cuts` groups
  bin = cut(predicted_prob_best_subset, breaks = num_cuts),  
  # observed values (abst outcome in test data)
  class = as.numeric(test_outcome)-1  
)

calib_data <- calib_data %>%
  group_by(bin) %>%
  summarise(
    observed = mean(class),  
    predicted = mean(prob),  
    se = sqrt(observed * (1 - observed) / n())  # Standard error
  )

# Add Loess Fit for Flexible Calibration Line
loess_fit <- loess(observed ~ predicted, data = calib_data, span = 0.75)
calib_data$loess_pred <- predict(loess_fit, calib_data$predicted)

# Plot Calibration Curve with Error Bars
calib_error_bar_best_subset = ggplot(calib_data) + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  geom_errorbar(aes(x = predicted, ymin = observed - 1.96 * se, 
                    ymax = observed + 1.96 * se), 
                colour="black", width=.01)+
  geom_point(aes(x = predicted, y = observed)) +
  labs(x = "Expected Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  theme_minimal()


# Plot Calibration Curve with Loess
calib_data <- calib_data %>%
  mutate(loess_ci_lower = loess_pred - 1.96 * sd(loess_pred),
         loess_ci_upper = loess_pred + 1.96 * sd(loess_pred))

calib_loess_best_subset = ggplot(calib_data, aes(x = predicted, y = observed)) +
  # Flexible calibration (Loess)
  geom_line(aes(y = loess_pred), color = "blue", linetype = "dashed") +  
  geom_ribbon(aes(ymin = loess_ci_lower, ymax = loess_ci_upper), alpha = 0.2, fill = "grey") +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Perfect calibration line
  labs(x = "Predicted Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r, fig.width = 10, fig.height = 10}
plots_stepwise = arrangeGrob(
  calib_error_bar_stepwise, calib_loess_stepwise, ROC_stepwise,
  ncol = 3,
  top = textGrob("Stepwise Selection",
                 gp = gpar(fontface = "bold", fontsize = 13)
))

plots_enet = arrangeGrob(
  calib_error_bar_enet, calib_loess_enet, ROC_enet,
  ncol = 3,
  top = textGrob("Elastic Net",
                 gp = gpar(fontface = "bold", fontsize = 13)
))

plots_best_subset = arrangeGrob(
  calib_error_bar_best_subset, calib_loess_best_subset, ROC_best_subset,
  ncol = 3,
  top = textGrob("Best Subset Selection", 
                 gp = gpar(fontface = "bold", fontsize = 13)
))

# Bold the main title
main_title <- textGrob(
  "Figure 1: Calibration Plots with Error Bars and LOESS and ROC Curves (Moderator Effects)",
  gp = gpar(fontsize = 16) 
)

# Arrange everything with the bold title
grid.arrange(
  plots_stepwise,
  plots_enet,
  plots_best_subset,
  nrow = 3,
  top = main_title
)
```

```{r}
test_outcome_numeric = as.numeric(test_outcome) - 1

# Calculate Brier Score for each model
brier_stepwise <- mean((predicted_prob_stepwise - test_outcome_numeric)^2)
brier_enet <- mean((predicted_prob_enet - test_outcome_numeric)^2)
brier_best_subset <- mean((predicted_prob_best_subset - test_outcome_numeric)^2)


# calibration error
ce_calculate <- function(predictions, actuals, n_bins = 10) {
  bins <- cut(predictions, breaks = seq(0, 1, length.out = n_bins + 1), 
              include.lowest = TRUE)
  bin_means <- tapply(predictions, bins, mean)
  bin_actuals <- tapply(actuals, bins, mean)
  bin_weights <- table(bins) / length(predictions)
  ce <- sum(bin_weights * abs(bin_means - bin_actuals))
  
  # Remove NA values from bin_means and bin_actuals
  valid_bins <- !is.na(bin_means) & !is.na(bin_actuals)
  bin_means <- bin_means[valid_bins]
  bin_actuals <- bin_actuals[valid_bins]
  bin_weights <- bin_weights[valid_bins]
  
  # Calculate ECE with valid bins only
  CE <- sum(bin_weights * abs(bin_means - bin_actuals))
  
  return(CE)
}

CE_stepwise <- ce_calculate(predicted_prob_stepwise, test_outcome_numeric)
CE_enet <- ce_calculate(predicted_prob_enet, test_outcome_numeric)
CE_best_subset <- ce_calculate(predicted_prob_best_subset, test_outcome_numeric)


# Calculate AUC for each model
auc_stepwise <- roc_stepwise$auc
auc_enet <- roc_enet$auc
auc_best_subset <- roc_best_subset$auc

# Get optimal threshold, specificity, and sensitivity for each model
threshold_stepwise <- as.numeric(coords(roc_stepwise, "best", 
                                        ret = "threshold"))
specificity_stepwise <- as.numeric(coords(roc_stepwise, "best", 
                                          ret = "specificity"))
sensitivity_stepwise <- as.numeric(coords(roc_stepwise, "best", 
                                          ret = "sensitivity"))

threshold_enet <- as.numeric(coords(roc_enet, "best", 
                                    ret = "threshold"))
specificity_enet <- as.numeric(coords(roc_enet, "best", 
                                      ret = "specificity"))
sensitivity_enet <- as.numeric(coords(roc_enet, "best", 
                                      ret = "sensitivity"))

threshold_best_subset <- as.numeric(coords(roc_best_subset, "best", 
                                           ret = "threshold"))
specificity_best_subset <- as.numeric(coords(roc_best_subset, "best", 
                                             ret = "specificity"))
sensitivity_best_subset <- as.numeric(coords(roc_best_subset, "best", 
                                             ret = "sensitivity"))

# Combine metrics into a table
df_performance <- rbind(
  `Brier score` = round(c(brier_stepwise, brier_enet, brier_best_subset), 4),
  `Calibration error` = round(c(CE_stepwise, CE_enet, CE_best_subset), 4),
  AUC = round(c(auc_stepwise, auc_enet, auc_best_subset), 4),
  Threshold = round(c(threshold_stepwise, threshold_enet, threshold_best_subset), 4),
  Specificity = round(c(specificity_stepwise, specificity_enet, specificity_best_subset), 4),
  Sensitivity = round(c(sensitivity_stepwise, sensitivity_enet, sensitivity_best_subset), 4)
  #`Adjusted R^2` = round(c(adj_r2_stepwise, adj_r2_enet, adj_r2_best_subset), 4)
)

# rename columns
colnames(df_performance) <- c("Stepwise", "Elastic Net", "Best Subset")

# Display the final table
kable(
  df_performance, 
  caption = "Calibration and Discrimination Metrics Moderator Effects Modeling"
)
```


## Baseline Characteristics as Potential Predictors
```{r}
predictor_names <- c("Var", "BA", "age_ps", "sex_ps", "inc", "edu_merged", "race",
                     "ftcd_score", "ftcd.5.mins", "bdi_score_w00", "cpd_ps",
                     "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",
                     "shaps_score_pq1", "otherdiag", "antidepmed", "mde_curr",
                     "NMR", "Only.Menthol", "readiness")
predictors <- data_imp[, predictor_names]
# for Lasso (to break down factors with >2 levels)
predictors_dummy <- model.matrix(~ 0 + ., data = predictors)
# remove the extra reference group
predictors_dummy <- predictors_dummy[, -which(colnames(predictors_dummy) =="Var0")]


# Full formula for main effects and interactions
full_formula <- as.formula(paste("abst ~", main_effects))

# Define scope with `Var` and `BA` as forced terms in the main model
scope_list <- list(
  lower = as.formula("abst ~ Var + BA"),  # Minimal model with controlled terms
  upper = full_formula                    # Full model with all main terms
)

# Fit logistic regression model with stepwise selection
predictor_stepwise_model <- step(
  glm(formula = abst ~ Var + BA, data = train_data, family = "binomial"),
  scope = scope_list,
  direction = "both",
  trace = 0
)
```

```{r}
# Fit Elastic Net
# To identify the potential interaction terms for moderator effects
train_predictors_dummy_include_names <- c(
  "Var1", "BA1", "age_ps", "sex_ps2", "inc2", "inc3",
  "inc4", "inc5", "edu_merged2", "edu_merged3",
  "raceBlack", "raceHispanic", "raceOther",
  "ftcd_score", "ftcd.5.mins1", "bdi_score_w00", "cpd_ps",
  "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",
  "shaps_score_pq1", "otherdiag1", "antidepmed1",
  "mde_curr1", "NMR", "Only.Menthol1",
  "readiness"
)

train_predictors_dummy <- predictors_dummy[train_index, ]
test_predictors_dummy <- predictors_dummy[-train_index, ]

train_predictors_dummy_include = 
  train_predictors_dummy[,train_predictors_dummy_include_names]
test_predictors_dummy_include = 
  test_predictors_dummy[,train_predictors_dummy_include_names]

# Set penalty factors to enforce keeping Var and BA
# Initialize penalty factors to 1 for all variables
penalty_factors <- rep(1, ncol(train_predictors_dummy_include))

# Identify columns corresponding exactly to "Var1" and "BA1" (not their interactions)
var1_col <- grep("^Var1$", colnames(train_predictors_dummy_include))
ba1_col <- grep("^BA1$", colnames(train_predictors_dummy_include))

penalty_factors[c(var1_col, ba1_col)] <- 0
names(penalty_factors) <- colnames(train_predictors_dummy_include)


set.seed(2024)
predictor_enet_model <- cv.glmnet(as.matrix(train_predictors_dummy_include), 
                                  train_outcome,
                         penalty.factor = penalty_factors,
                         alpha = 0.5, family = "binomial")

# Extract coefficients at the optimal lambda (best_lambda)
best_lambda_enet <- predictor_enet_model$lambda.min
#remove intercept
optimal_coefs_enet <- as.numeric(coef(predictor_enet_model, s = best_lambda_enet)[-1])
coef_names_enet <- rownames(coef(predictor_enet_model, s = best_lambda_enet))[-1]

predictor_result_table_enet <- data.frame(
  variable = coef_names_enet,
  Coefficient = optimal_coefs_enet
) %>%
  filter(Coefficient != 0)
```

```{r}
predictor_regsubsets_model <-
    regsubsets(y = train_outcome,
               x = train_predictors_dummy_include,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = 20,    
               force.in = c("Var1", "BA1"),
               force.out = NULL,
               really.big = T,
               method = "seqrep")

predictor_reg_summary = summary(predictor_regsubsets_model)
cp_min = which.min(predictor_reg_summary$cp)
predictor_best_subset_coefs = coef(predictor_regsubsets_model, cp_min)
predictor_best_subset_names = names(coef(predictor_regsubsets_model, cp_min))
```


```{r}
# Summary table of coef
# Stepwise coefficients
stepwise_coefs <- coef(predictor_stepwise_model)
stepwise_df <- data.frame(
  variable = names(stepwise_coefs),
  `Stepwise` = as.numeric(stepwise_coefs)
)

# ENET coefficients 
enet_df <- predictor_result_table_enet %>%
  rename(`Elastic Net` = Coefficient)

# Best subset coefficients
best_subset_df <- data.frame(
  variable = names(predictor_best_subset_coefs),
  `Best Subset` = as.numeric(predictor_best_subset_coefs)
)

# Merge all into one table based on variable names
combined_df <- full_join(stepwise_df, enet_df, by = "variable") %>%
  full_join(best_subset_df, by = "variable") %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

# Remove the intercept row
combined_df <- combined_df[combined_df$variable != "(Intercept)", ]

# Replace NA values with an empty space
combined_df[is.na(combined_df)] <- " "

colnames(combined_df) <- gsub("Best.Subset", "Best Subset", colnames(combined_df))

# enforce predictor print-out order
combined_df <- combined_df %>%
  arrange(
    factor(variable, levels = c("BA1", "Var1")),  # Keep BA1 and Var1 at the top
    variable                                      # Sort the rest alphabetically
  )

# Display the final combined table
kable(combined_df, row.names = F,
      caption = "Summary of Coefficients for Potential Predictor Effects 
      across Model Selection Methods")
```

```{r}
predictor_prob_stepwise <- as.numeric(predict(predictor_stepwise_model,
                                newdata = test_data,
                                type = "response"))

# Plot ROC and calculate AUC
predictor_roc_stepwise <- roc(test_outcome, predictor_prob_stepwise)

roc_data <- data.frame(
  Specificity = rev(predictor_roc_stepwise$specificities),
  Sensitivity = rev(predictor_roc_stepwise$sensitivities)
)

auc_value <- auc(predictor_roc_stepwise)

# Plot ROC curve with ggplot2
predictor_ROC_stepwise = ggplot(roc_data, aes(x = 1-Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, 
           label = paste("AUC =", round(auc_value, 2)), size = 5, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5))
```


```{r}
predictor_prob_enet <- as.numeric(predict(predictor_enet_model, 
                                newx = as.matrix(test_predictors_dummy), 
                                s = "lambda.min", type = "response"))

# Plot ROC curve and calculate AUC
predictor_roc_enet <- roc(test_outcome, predictor_prob_enet)

# Convert the ROC object to a data frame for ggplot2
roc_data <- data.frame(
  Specificity = rev(predictor_roc_enet$specificities),
  Sensitivity = rev(predictor_roc_enet$sensitivities)
)

auc_value <- auc(predictor_roc_enet)

# Plot ROC curve with ggplot2
predictor_ROC_enet = ggplot(roc_data, aes(x = 1-Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, label = paste("AUC =", round(auc_value, 2)), size = 5, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```


```{r}
predictor_prob_best_subset <-  as.numeric(
  predict_best_subset(
    train_data = cbind(train_predictors_dummy, 
                       abst = as.numeric(train_outcome) -1),
    test_data = cbind(test_predictors_dummy, 
                      abst = as.numeric(test_outcome) -1),
    predictor_best_subset_coefs
  )
)

# Plot ROC and calculate AUC
predictor_roc_best_subset <- roc(test_outcome, predictor_prob_best_subset)

roc_data <- data.frame(
  Specificity = rev(predictor_roc_best_subset$specificities),
  Sensitivity = rev(predictor_roc_best_subset$sensitivities)
)

auc_value <- auc(predictor_roc_best_subset)

# Plot ROC curve with ggplot2
predictor_ROC_best_subset = ggplot(roc_data, aes(x = 1-Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, 
           label = paste("AUC =", round(auc_value, 2)), size = 5, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5))
```

```{r}
# Calculate Brier Score for each model
brier_stepwise <- mean((predictor_prob_stepwise - test_outcome_numeric)^2)
brier_enet <- mean((predictor_prob_enet - test_outcome_numeric)^2)
brier_best_subset <- mean((predictor_prob_best_subset - test_outcome_numeric)^2)


# calibration error
ce_calculate <- function(predictions, actuals, n_bins = 10) {
  bins <- cut(predictions, breaks = seq(0, 1, length.out = n_bins + 1), 
              include.lowest = TRUE)
  bin_means <- tapply(predictions, bins, mean)
  bin_actuals <- tapply(actuals, bins, mean)
  bin_weights <- table(bins) / length(predictions)
  ce <- sum(bin_weights * abs(bin_means - bin_actuals))
  
  # Remove NA values from bin_means and bin_actuals
  valid_bins <- !is.na(bin_means) & !is.na(bin_actuals)
  bin_means <- bin_means[valid_bins]
  bin_actuals <- bin_actuals[valid_bins]
  bin_weights <- bin_weights[valid_bins]
  
  # Calculate ECE with valid bins only
  CE <- sum(bin_weights * abs(bin_means - bin_actuals))
  
  return(CE)
}

CE_stepwise <- ce_calculate(predictor_prob_stepwise, 
                                      test_outcome_numeric)
CE_enet <- ce_calculate(predictor_prob_enet, 
                                  test_outcome_numeric)
CE_best_subset <- ce_calculate(predictor_prob_best_subset, 
                                         test_outcome_numeric)
```


```{r}
# Calculate AUC for each model
auc_stepwise <- predictor_roc_stepwise$auc
auc_enet <- predictor_roc_enet$auc
auc_best_subset <- predictor_roc_best_subset$auc

# Get optimal threshold, specificity, and sensitivity for each model
threshold_stepwise <- as.numeric(coords(predictor_roc_stepwise, "best", 
                                        ret = "threshold"))
specificity_stepwise <- as.numeric(coords(predictor_roc_stepwise, "best", 
                                          ret = "specificity"))
sensitivity_stepwise <- as.numeric(coords(predictor_roc_stepwise, "best", 
                                          ret = "sensitivity"))

threshold_enet <- as.numeric(coords(predictor_roc_enet, "best", 
                                    ret = "threshold"))
specificity_enet <- as.numeric(coords(predictor_roc_enet, "best", 
                                      ret = "specificity"))
sensitivity_enet <- as.numeric(coords(predictor_roc_enet, "best", 
                                      ret = "sensitivity"))

threshold_best_subset <- as.numeric(coords(predictor_roc_best_subset, "best", 
                                           ret = "threshold"))
specificity_best_subset <- as.numeric(coords(predictor_roc_best_subset, "best", 
                                             ret = "specificity"))
sensitivity_best_subset <- as.numeric(coords(predictor_roc_best_subset, "best", 
                                             ret = "sensitivity"))

# Combine metrics into a table
predictor_df_performance <- rbind(
  `Brier score` = round(c(brier_stepwise, brier_enet, brier_best_subset), 4),
  `Calibration error` = round(c(CE_stepwise, CE_enet, CE_best_subset), 4),
  AUC = round(c(auc_stepwise, auc_enet, auc_best_subset), 4),
  Threshold = round(c(threshold_stepwise, threshold_enet, threshold_best_subset), 4),
  Specificity = round(c(specificity_stepwise, specificity_enet, specificity_best_subset), 4),
  Sensitivity = round(c(sensitivity_stepwise, sensitivity_enet, sensitivity_best_subset), 4)
  #`Adjusted R^2` = round(c(adj_r2_stepwise, adj_r2_enet, adj_r2_best_subset), 4)
)

# rename columns
colnames(predictor_df_performance) <- c("Stepwise", "Elastic Net", "Best Subset")

# Display the final table
kable(
  predictor_df_performance, 
  caption = "Calibration and Discrimination Metrics for Predictor Effect Modeling"
)
```


# \textcolor{orange}{CONCLUSION}

# \textcolor{orange}{LIMITATIONS}

# Consent, Data, and Code Availability

Primary data were provided by Dr. George Papandonatos from the Department of Biostatistics at Brown University. The original data cannot be shared directly for privacy. Replication scripts are available at <https://github.com/YanweiTong-Iris/PHP2550-Fall24/tree/main/Project2>.

# Reference
<div id="refs"></div>



\pagebreak

# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
