---
title: "TBD"
subtitle: "PHP2550 Project 3: A simulation study"
author: "Yanwei (Iris) Tong"
date: "2024-11-12"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
    #number_sections: true
link-citations: yes
header-includes:
  - "\\usepackage{xcolor}"
  - "\\usepackage{amsmath}"
abstract: |
  **Purpose:** 
  
  **Methods**:     
  
  **Results and conclusion**: 
  

geometry: margin=1in
fontsize: 10.5pt
---

```{r setup, include=FALSE}
# to prevent scientific notation
options(scipen=999)

# Set up knit environment
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      error = FALSE)

# Load necessary packages
library(tidyverse)
library(kableExtra)
library(knitr)
library(ggplot2)
library(gridExtra)
library(grid)
library(naniar)
library(gtsummary)
library(gt)
library(patchwork)
library(knitcitations)
library(glmnet)
library(pROC)
library(MASS)
library(leaps)
library(RColorBrewer) 
library(cowplot)
library(lme4)
library(doParallel)
library(foreach)


# Define data saving paths
data_path = paste0(here::here(), "/Project3/simulated_data")
result_path = paste0(here::here(), "/Project3/results")
```

# \textcolor{orange}{INTRODUCTION}

# \textcolor{orange}{SIMULATION DESIGN}

## Aims and Objectives

**Aim 1**: To evaluate the impact of different design choices (number of clusters $G$ and observations per cluster $R$) under budget constraints $B$.

**Aim 2**: To explore the relationships between the underlying data generation mechanism parameters (e.g., variance components) and cost structure (relative costs $c_1/c_2$) and their impacts on estimation efficiency of treatment effects.

**Aim 3**: To compare performance under different outcome distributions (Normal vs. Poisson).

## Data Generating Mechanisms

#### Cost constraint

$$
c_1 + G(R-1)c_2 \leq B
$$

#### Hierarchical Model for Normal Outcomes

1.  **Cluster-level mean:**

$$
\mu_{i0} = \alpha + \beta X_i \quad (\text{fixed effect for treatment/control groups}) \\
\mu_i \mid \epsilon_i = \mu_{i0} + \epsilon_i, \quad \epsilon_i \sim N(0, \gamma^2) \\
\mu_i \sim N(\mu_{i0}, \gamma^2)
$$

2.  **Observation-level outcomes:**

$$
Y_{ij} \mid \mu_i = \mu_i + e_{ij}, \quad e_{ij} \sim N(0, \sigma^2)\\
Y_{ij} \mid \mu_i \sim N(\mu_i, \sigma^2)$$

3.  **Marginal outcome distribution:**
$$\text{Marginal mean:} \quad \mathbb{E}[Y\_{ij} \mid X_i] = \alpha + \beta X_i \\
\text{Marginal variance:} \quad \text{Var}(Y\_{ij} \mid X_i) = \gamma^2 + \sigma^2 $$

#### Hierarchical Model for Poisson Outcomes

1.  **Cluster-level mean:**
$$
\log(\mu_i) = \alpha + \beta X_i + \epsilon_i, \quad \epsilon_i \sim N(0, \gamma^2)\\
\mu_i \sim \text{LogNormal}(\alpha + \beta X_i, \gamma^2)
$$

2.  **Observation-level outcomes:** 
$$
    Y_{ij} \mid \mu_i \sim \text{Poisson}(\mu_i)
$$

3.  **Summing within a cluster:** 
$$
Y_i = \sum_{j=1}^R Y_{ij} \quad (\text{sum of all observations in the cluster}) 
$$ 
$$
Y_i \mid \mu_i \sim \text{Poisson}(R \mu_i)
$$



```{r}
#' @param alpha Intercept parameter (baseline mean).
#' @param beta Treatment effect.
#' @param gamma_sq Variance of cluster-level random effects.
#' @param sigma_sq Variance of observation-level random noise.
#' @param B Total budget available for the study (in dollars).
#' @param c1 Cost of the first sample in each cluster.
#' @param c2 Cost of additional samples in the same cluster (c2 < c1).
#' @param G Number of clusters
#' @param R Number of observations per cluster
#' @param n_sim Number of simulation iterations for tracking purposes
#' @return Data frame containing simulated data (Y, X) and the input parameters.
#' @export
simulate_clustered_data <- function(model = "normal",
                                    alpha,
                                    beta,
                                    gamma_sq,
                                    sigma_sq,
                                    B,
                                    c1,
                                    c2,
                                    G,
                                    R,
                                    n_sim = 1) {
  
  X <- rbinom(G, size = 1, prob = 0.5)
  epsilon <- rnorm(G, mean = 0, sd = sqrt(gamma_sq))
  
  Y_list <- list()
  cluster_id <- c()
  observation_id <- c()
  n_sim_id <- c()
  
  for (i in 1:G) {
    mu_i <- alpha + beta * X[i] + epsilon[i]
    e_ij <- rnorm(R, mean = 0, sd = sqrt(sigma_sq))
    Y_ij <- mu_i + e_ij
    
    Y_list[[i]] <- Y_ij
    cluster_id <- c(cluster_id, rep(i, R))
    observation_id <- c(observation_id, 1:R)
    n_sim_id <- c(n_sim_id, rep(n_sim, R))
  }
  
  Y <- unlist(Y_list)
  X_obs <- rep(X, each = R)
  data <- data.frame(Y, X_obs, cluster_id, observation_id, n_sim = n_sim_id)
  return(data)
}


#' Find optimal G and R to minimize variance
#'
#' @param alpha Intercept parameter (baseline mean).
#' @param beta Treatment effect.
#' @param gamma_sq Variance of cluster-level random effects.
#' @param sigma_sq Variance of observation-level random noise.
#' @param B Total budget available for the study (in dollars).
#' @param c1 Cost of the first sample in each cluster.
#' @param c2 Cost of additional samples in the same cluster (c2 < c1).
#' @param n_simulations Number of simulations to run for each G and R combination.
#' @print List with optimal G, optimal R, and minimum variance.
#' @return The result dataset with mean and var of Beta_hat for all G and R combination
#' @export
find_optimal_G_R <- function(alpha, beta, gamma_sq, sigma_sq, B, c1, c2, n_simulations = 100) {
  min_variance <- Inf
  best_G <- 0
  best_R <- 0
  
  results <- data.frame(alpha = numeric(), beta = numeric(), 
                        gamma_sq = numeric(), sigma_sq = numeric(), 
                        B = numeric(), c1 = numeric(), c2 = numeric(),
                        G = integer(), R = integer(), 
                        betahat = numeric(), variance = numeric())
  
  for (G in 2:(floor((B / c1)) - 1)) {
    R = floor((B - G * c1) / (c2 * G)) + 1
    if (R > 1) {
      betahats <- numeric(n_simulations)
      
      data_list <- list()
      
      set.seed(46)
      for (sim in 1:n_simulations) {
        data <- simulate_clustered_data(model = "normal", 
                                        alpha, beta, 
                                        gamma_sq, sigma_sq, 
                                        B, c1, c2, 
                                        G, R, n_sim = sim)
        
        model <- lmer(Y ~ X_obs + (1 | cluster_id), data = data)
        betahats[sim] <- fixef(model)["X_obs"]
        data_list[[sim]] <- data
      }
      
      # Combine all simulated datasets for this G and R combination
      combined_data <- do.call(rbind, data_list)
      saveRDS(combined_data, paste0(data_path, "/beta_", beta, "_gammasq_", gamma_sq, 
                                    "_sigmasq_", sigma_sq,
                                    "_B_", B, "_c1_", c1, "_c2_", c2,
                                    "/simulated_data_G_", G, "_R_", R, ".rds"))
      
      # Remove NA values from betahats
      betahats <- betahats[!is.na(betahats)]
      
      betahat_mean <- mean(betahats)
      betahat_variance <- var(betahats)
      

    if (betahat_variance < min_variance) {
        min_variance <- betahat_variance
        best_G <- G
        best_R <- R
      }
    
    results <- rbind(
      results,
      data.frame(
        alpha = alpha,
        beta = beta,
        gamma_sq = gamma_sq,
        sigma_sq = sigma_sq,
        B = B,
        c1 = c1,
        c2 = c2,
        G = G,
        R = R,
        betahat_mean = betahat_mean,
        betahat_var = betahat_variance
      )
    )
  }}
  
  write.csv(
    results,
    paste0(
      result_path,
      "/simulation_results_all_beta_", beta, "_gammasq_", gamma_sq, 
      "_sigmasq_", sigma_sq,"_B_", B, "_c1_", c1, "_c2_", c2, ".csv"
    ),
    row.names = FALSE
  )
  return(results)
  cat("Minimum betahat variance:", min_variance, "\n")
  cat("Optimal number of clusters (G):", best_G, "\n")
  cat("Optimal number of observations per cluster (R):", best_R, "\n")
}


#' Make the point plot for Var(Betahat) vs. G
#'
#' @param result The result dataset
#' @print The point plot with Var(Betahat) vs G.
var_vs_G_plot <- function(result){
  beta = unique(result[["beta"]])
  B = unique(result[["B"]])
  c1 = unique(result[["c1"]])
  c2 = unique(result[["c2"]])
  # Find the point with the lowest betahat_var
lowest_point <- result[which.min(result$betahat_var), ]

ggplot(result, aes(x = G, y = betahat_var)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  geom_point(data = lowest_point, aes(x = G, y = betahat_var), color = "red", size = 3) +
  geom_text(data = lowest_point, aes(x = G, y = betahat_var, label = round(betahat_var, 2)), 
            vjust = -1, color = "red") +
  labs(title = "Relationship between G and Var(Betahat)",
       subtitle = paste("(B =", B, ", c1 =", c1, ", c2 =", c2, 
                         ", True Beta =", beta, ")"),
       x = "G",
       y = "Var(Betahat)") +
  theme_minimal()
}
```


```{r, fig.width = 6, fig.height = 4, message = F, warning = F}
alpha_list <- c(10, 10, 10, 5, 5, 5, 0, 0, 0)
beta_list <- c(2, 2, 2, 1, 1, 1, 3, 3, 3)
gamma_sq_list <- c(4, 4, 4, 4, 4, 4, 1, 1, 1)
sigma_sq_list <- c(1, 1, 1, 4, 4, 4, 4, 4, 4)
B_list <- c(3000, 3000, 3000, 2000, 2000, 2000, 5000, 5000, 5000)
c1_list <- c(100, 50, 50, 100, 50, 50, 100, 50, 50)
c2_list <- c(5, 5, 10, 5, 5, 10, 5, 5, 10)

cl <- makeCluster(4)
registerDoParallel(cl)

#for (i in 1:9){
foreach (i = 1:9) %dopar%{
  alpha = alpha_list[[i]]
  beta = beta_list[[i]]
  gamma_sq = gamma_sq_list[i]
  sigma_sq = sigma_sq_list[i]
  B = B_list[i]
  c1 = c1_list[i]
  c2 = c2_list[i]
  result <- find_optimal_G_R(alpha = alpha, beta = beta, 
                             gamma_sq = gamma_sq, sigma_sq = sigma_sq, 
                             B = B, c1 = c1, c2 = c2,
                             n_simulations = 100)
  View(result)
  var_vs_G_plot(result)
}

stopCluster(cl)
```




## Estimands

## Methods to Evaluate

## Performance Measures

# \textcolor{orange}{RESULTS}

# \textcolor{orange}{CONCLUSION}

# \textcolor{orange}{LIMITATIONS}

# Data, and Code Availability

This project is a collaboration with Dr. Zhijin Wu in the Biostatistics Department. Replication scripts and simulated data are available at <https://github.com/YanweiTong-Iris/PHP2550-Fall24/tree/main/Project2>.

# Reference

\pagebreak

# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
